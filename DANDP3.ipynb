{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DANDP3\n",
    "## Wrangling Data from OSM with MongoDB\n",
    "\n",
    "####  Data Analyst Nanodegree\n",
    "\n",
    "#### Anna Signor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Overview\n",
    "\n",
    "\n",
    "I followed the folowing steps:\n",
    "\n",
    "\n",
    "* download XML from Open Street Map, using mapzen\n",
    "* with Python scripts check for problems in the data\n",
    "* adjust code accordingly\n",
    "* parse and shape data into one JSON file\n",
    "* import data into MongoDB\n",
    "* through datbase queries, check for remaining problems\n",
    "* repeat last 4 steps until data is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area \n",
    "\n",
    "The area chosen is São Paulo, Brazil, where I was born and raised. The XML dataset was downloaded from MapZen. My intention was to download and explore the São Paulo Metropolitan Area, [this map relation](http://www.openstreetmap.org/relation/2661855#map=9/-23.6242/-46.4510). As is detailed in a further section, I found that this is a rather ambiguous term, there being at least two different concepts that translate into that from Portuguese. The conclusion is the area analysed is called [*Complexo Metropolitano Expandido*](https://pt.wikipedia.org/wiki/Complexo_Metropolitano_Expandido) or [Expanded Metropolitan Complex of São Paulo](https://en.wikipedia.org/wiki/Expanded_Metropolitan_Complex_of_S%C3%A3o_Paulo), a considerably larger area than what I initially thought I had downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Problems Uncovered Before Querying DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Street Names\n",
    "Using an **audit** function two distinct classes of problems were uncovered with the street names:\n",
    "\n",
    "**  street types uppper/lower case or abbreviations inconsistencies, mispellings: ** \n",
    "E. g. the words \"Rua\", \"R.\", \"RUA\", \"Rue\" and \"rua\" all occurred\n",
    "\n",
    "\n",
    "\n",
    "**  street types mising : **\n",
    "E. g. \"Alfonso Bovero\" where \"Avenida Alfonso Bovero\" should be\n",
    "\n",
    "Both issues were addressed by the function **improve_names_BR** below. Information elucidaded by **audit** was fed back into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = {'avenida':'Avenida', #mapping to fix case mispellings, case, abbreviations\n",
    "           u'Al.': 'Alameda',\n",
    "           'Rue': 'Rua',\n",
    "           u'Av.': 'Avenida',\n",
    "           u'Av': 'Avenida',\n",
    "           'RUa': 'Rua',\n",
    "           'R': 'Rua',\n",
    "           'Acost.': 'Acostamento',\n",
    "           'RUA': 'Rua',\n",
    "           'rua' : 'Rua',\n",
    "           'R.' : 'Rua',\n",
    "           'AC': 'Acesso',\n",
    "           'estrada' : 'Estrada',\n",
    "           'travessa' : 'Travessa'           \n",
    "           } \n",
    "good_types = set(['Acostamento', #Set of acceptable street types\n",
    "                  u'Pra\\xe7a', \n",
    "                  'Alameda', \n",
    "                  'Viela', \n",
    "                  'Estrada', \n",
    "                  'Rua', \n",
    "                  'Acesso', \n",
    "                  'Parque', \n",
    "                  'Largo', \n",
    "                  'Via', \n",
    "                  'Marginal', \n",
    "                  'Rodovia', \n",
    "                  'Corredor', \n",
    "                  'Viaduto', \n",
    "                  'Travessa', \n",
    "                  'Pateo', \n",
    "                  'Avenida', \n",
    "                  'Passagem',\n",
    "                  u'Complexo Vi\\xe1rio'])\n",
    "mapping2 = {u'1\\xaa Travessa da Estrada do Morro Grande' : '', #mapping for case when street type is missing\n",
    "            'Alfonso Bovero' : 'Avenida',                      #this was manually created by looking up all the names\n",
    "            u'N\\xedvia Maria Dombi' : 'Travessa'               #the ones not showing here are \"Rua\" types\n",
    "            }\n",
    "\n",
    "good_tuple = tuple(good_types)\n",
    "\n",
    "def improve_name_BR(name):\n",
    "    \"\"\"takes a street name from sao paulo and returns improved name\"\"\"\n",
    "    words = name.split()    \n",
    "    if name.startswith(good_tuple):\n",
    "        return name\n",
    "        ### if name is okay, return name (do nothing)\n",
    "    elif words[0] in mapping:\n",
    "        words[0] = mapping[words[0]]        \n",
    "        return ' '.join(words)\n",
    "        ### if type is mispelled or miscased, update 1st word of name and return joined string\n",
    "    elif name in mapping2:\n",
    "        return mapping2[name] + ' ' + name\n",
    "        ### if name is one of the odd cases but not needing word 'Rua', use mapping2 to fix\n",
    "    else:\n",
    "        return 'Rua' + ' ' + name\n",
    "        ### the cases left are the ones where the word 'Rua' was left out\n",
    "        ### this choice was made because \"Rua\" is the most commonly occurring type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Structure\n",
    "The data structure was interesting. It may be adequate for OSM, but it is certainly not how I would like for it to figure in the MongoDB collection. Some information is represented directly as attributes of a main XML data primitive element, while others, as attributes of child elements tagged \"tag\". A choice was made by OSM to have attributes called \"k\" and \"v\", the values of which represent keys and values, rather than using an XML of the type```<key> value <key/>```. Furthermore, there was use of a colon hierarquical structure in some of the \"k\"s. Here is an example node:\n",
    "```\n",
    "<node changeset=\"38648623\" id=\"4128352041\" lat=\"-23.5591903\" lon=\"-46.6587486\" timestamp=\"2016-04-17T17:59:02Z\" uid=\"2030995\" user=\"Bonix-Mapper\" version=\"1\">\n",
    "\t\t<tag k=\"name\" v=\"Banca Paulista V\" />\n",
    "\t\t<tag k=\"shop\" v=\"books\" />\n",
    "\t\t<tag k=\"phone\" v=\"+55 11 3288-8241\" />\n",
    "\t\t<tag k=\"addr:street\" v=\"Avenida Paulista\" />\n",
    "\t</node>\n",
    "```\n",
    "I would like to represent the data in JSON in a different shape, so this was something that had to be considered while parsing. To put it simply: although a very simple and straight-forward mapping to translate XML into JSON is always possible, this is not what I used because the data was not in a desireable shape. This is what the function **shape_element** mostly does. \n",
    "\n",
    "The shape chosen to represent the data is the following:\n",
    "```\n",
    "{\n",
    "\"id\": value,\n",
    "\"data_prim\": node_way_or_relation,\n",
    "\"visible\":true_or_false,\n",
    "\"created\": {\n",
    "          \"version\":value,\n",
    "          \"changeset\":value,\n",
    "          \"timestamp\":value,\n",
    "          \"user\":value,\n",
    "          \"uid\":value_for_user\n",
    "        },\n",
    "\"pos\": [LAT, LON],\n",
    "\"address\": {\n",
    "          \"housenumber\": value,\n",
    "          \"postcode\": value,\n",
    "          \"street\": value\n",
    "           ...\n",
    "        },\n",
    "\"amenity\": value,\n",
    "\"cuisine\": value,\n",
    "\"name\": value,\n",
    "\"phone\": value\n",
    "\"any_other_attrib_1\": value\n",
    "...\n",
    "\"any_other_attrib_n\": value\n",
    "\"A\" : {\n",
    "        \"B\" : { \n",
    "                \"C\" : value               \n",
    "                }\n",
    "        }\n",
    "}\n",
    "```\n",
    "where \"A\", \"B\" and \"C\" represent the ```<tag k=\"A:B:C\" v=value />``` value situations that are not the address, which is a special case.\n",
    "\n",
    "Most of this was hard-coded into **shape_element**, and to deal with the colons I used a helper function called **smarter_nestify**. (Which actually allows for processing a key containing an arbitrary number of colons into nested dictionaries, using regression.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smarter_nestify(l, record):\n",
    "    \"\"\"Takes a list [a1, a2, a3, ... , an, value] and a pre-existing dictionary structure returns a nested dictionary object\n",
    "    {a1 : {a2 : {a3 : ... {an : value} ...}}}, respecting the pre-existing dictionary records, that is, for each recursion\n",
    "    step if a dictionary ai already exists it will add a key ai+1 to it rather than creating a new dictionary ai.\"\"\"    \n",
    "    if len(l) == 2: #if list is down to two elements [a, val], return {a : val} \n",
    "        key = l[0]\n",
    "        value = l[1]\n",
    "        return {key: value}\n",
    "    else:\n",
    "        key = l[0]\n",
    "        record[key] = smarter_nestify(l[1:], record.get(key, {}))                 \n",
    "        return record\n",
    "    # function pops the first element of the list, makes a dictionary {k : v} where k is the popped element and v is what is \n",
    "    # returned when calling itself on popped list and empty dictionary or existing one, depending on record\n",
    "    \"\"\"adapted from:                                              \n",
    "    http://stackoverflow.com/questions/37014500/how-to-use-recursion-to-nest-dictionaries-while-integrating-with-existing-record\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Repeated Attribute Keys\n",
    "This was an interesting issue that stems from not discarding the tags containing colons. \n",
    "The problem is tag 'k' attributes that have different functions being called the exact same in the XML data. \n",
    "For example, we had: \n",
    "```\n",
    "<tag 'k'='lanes:psv:forward' 'v'='1'> \n",
    "```\n",
    "and \n",
    "```\n",
    "<tag 'k'='lanes' 'v'='2'>\n",
    "```\n",
    "in the same node. In the OSM XML schema, the first data has to do with lanes that have special permission (like a taxi, carpool or a bus lane) while the second is simply the number of lanes in any road. This was causing a variable type error. It was resolved by repeatedly trapping the error and using the information back into the code manually. \n",
    "Ref: https://discussions.udacity.com/t/keep-attr-atrr-atrr-formatted-data/166864/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Variable types\n",
    "The data was almost entirely represented in strings or unicode. A lot of the data will be more useful a different variable type. MongoDB supports all the types supported in Python, and since some of the calculations are numerical, it will be in my favor to convert types as the data is parsed.\n",
    "Conversions made:\n",
    "\n",
    "\n",
    "* 'POS' into float\n",
    "\n",
    "\n",
    "* 'version' into int\n",
    "\n",
    "\n",
    "OBS: It is in my favor to treat postcode as a string since the zeroes to the left have significance, which an int type would ignore. Also, brazilian postcodes contain a non-numerical charachter \"-\", which would cause problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Problems Uncovered by DB Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is preparation for DB query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "client = MongoClient()\n",
    "db = client\n",
    "sp = db.my_osm.cme #shorthand since all the queries will be in same collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bad Key 'type' Creating Incorrect Parsing\n",
    "\n",
    "At first, the desired data format had a 'type' key in the main JSON node, to designate a map node, way or relation (please note the word \"node\" here has two very different meanings). So I ran the query below to find out how many relations were in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relations = sp.find({'type' : 'relation'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original output of this query was 4. It seems odd that a metropolis so big would have 4 relations. Investigating further, I found there were 261671 ways and 1900291 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6353"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 + 261671 + 1900291 - 2168319 #this should return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this effectively means is that there is some kind of discrepancy, and it is not small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'enforcement',\n",
      " u'palm',\n",
      " u'triangulation',\n",
      " u'fixed_point',\n",
      " u'multipolygon',\n",
      " u'site',\n",
      " u'water',\n",
      " u'oil',\n",
      " u'route',\n",
      " u'gas',\n",
      " u'audio',\n",
      " u'boundary',\n",
      " u'restriction',\n",
      " u'waterway',\n",
      " u'associatedStreet',\n",
      " u'route_master',\n",
      " u'public_transport',\n",
      " u'bridge',\n",
      " u'tunnel']\n"
     ]
    }
   ],
   "source": [
    "types = sp.distinct('type')\n",
    "pprint.pprint(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The above output is totally unexpected, as we should see only \"node\", \"relation\" or \"way\". A little research was helpful in pinpointing the issue, mainly that relation data primitive were translating into nodes with incorrect 'type' assignment, because many of the relations themselves had a 'type' attribute or tag child. Full explanation on [this reference](https://discussions.udacity.com/t/problem-cleaning-my-osm-dataset/35085/2). The forum post also contains the solution, which is to not use the word \"type\". I decided to call this key \"data_prim\" in short for \"data primitive\".\n",
    " \n",
    "After fixing the Python code, making a new JSON file, clearig the old collection off the database and loading the new one, when we run the queries below the expected outputs are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'node', u'way', u'relation']\n"
     ]
    }
   ],
   "source": [
    "types = sp.distinct('data_prim')\n",
    "pprint.pprint(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261695 ways\n",
      "1900322 nodes\n",
      "6302 relations\n",
      "discrepancy: 0\n"
     ]
    }
   ],
   "source": [
    "cursor = sp.find({'data_prim' : 'way'})\n",
    "a = len(list(cursor))\n",
    "print a, 'ways'\n",
    "cursor = sp.find({'data_prim' : 'node'})\n",
    "b = len(list(cursor))\n",
    "print b, 'nodes'\n",
    "cursor = sp.find({'data_prim' : 'relation'})\n",
    "c =  len(list(cursor))\n",
    "print c, 'relations'\n",
    "print 'discrepancy:', 2168319 - a - b - c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Seamarks and unexpected postcodes\n",
    "\n",
    "One of the strange things in my data was the occurrence of the tag \"seamark\". This is one of the features that make heavy use of the colons structure in the OSM XML schema, so it was in the back of my head. A simple query reveales how many of them there are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find({'seamark' : {'$exists' : 1}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why this is strange is upon further investigation in the OSM Wiki, I found these are features that should occur in oceanic coasts. Querying the data base for examples I found some were lighhouses and buoyes. The problem is the metropolitan area I was supposed to be analysing contains NO sea coast.\n",
    "The query below shows how many and which cities figure in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Aldeia de Carapicu\\xedba', u'Alum\\xednio', u'Ara\\xe7ariguama', u'Aruja', u'Aruj\\xe1', u'Atibaia', u'Barueri', u'Bertioga', u'Bom Jesus dos Perd\\xf5es', u'Cajamar', u'Campo Limpo Paulista', u'Carapicu\\xedba', u'Cotia', u'Cubat\\xe3o', u'Diadema', u'Embu das Artes', u'Engenheiro Goulart', u'Ferraz de Vasconcelos', u'Francisco Morato', u'Franco da Rocha', u'Guaruja', u'Guaruj\\xe1', u'Guarulhos', u'Ibi\\xfana-SP', u'Igarat\\xe1', u'Indaiatuba', u'Itanha\\xe9m', u'Itapacerica da Serra', u'Itapecerica da Serra', u'Itapevi', u'Itaquacetuba', u'Itaquaquecetuba', u'Itu', u'Itupeva', u'JUndia\\xed', u'Jacare\\xed', u'Jundiai', u'Jundia\\xcc', u'Jundia\\xed', u'Jundi\\xe1i', u'Mairipora', u'Mairipor\\xe3', u'Maua', u'Mau\\xe1', u'Mogi das Cruzes', u'Mogis das Cruzes', u'Mooca', u'Osasco', u'Po\\xe1', u'Ribeir\\xe3o Pires', u'SA\\xd5 PAULO', u'Salto', u'Santana de Parna\\xedba', u'Santo Andre', u'Santo Andr\\xe9', u'Santos', u'Sao Bernardo do Campo', u'Sao Jose dos Campos', u'Sao Paolo', u'Sao Paulo', u'Sao paulo', u'Suzano', u'S\\xc3O PAULO-SP', u'S\\xe0o Paulo', u'S\\xe2o Paulo', u'S\\xe3o Bernardo do Campo', u'S\\xe3o Bernardo do campo', u'S\\xe3o Caetano do Sul', u'S\\xe3o Jos\\xe9 dos Campos', u'S\\xe3o Pailo', u'S\\xe3o Paulo', u'S\\xe3o Paulo - SP', u'S\\xe3o Paulo, SP', u'S\\xe3o Paulo/SP', u'S\\xe3o Paulol', u'S\\xe3o Roque', u'S\\xe3o Vicente', u'S\\xe3o jos\\xe9 dos Campos', u'S\\xe3o paulo', u'S\\xe3o vicente', u'Tabo\\xe3o da Serra', u'Vargem Grande Paulista', u'Varzea Paulista', u'V\\xe1rzea Paulista', u'jd dos cunhas', u'jundia\\xed', u'santo Andr\\xe9', u'sao paulo', u'sp', u's\\xe3o Caetano do Sul', u's\\xe3o Paulo', u's\\xe3o paulo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = sp.distinct('address.city')\n",
    "\n",
    "cities.sort()\n",
    "print cities\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to indicate the area is, in fact, what is called Expanded Metropolipan Complex of São Paulo or *Complexo Metropolitano Expandido* (in this terminology \"São Paulo\" is implied), also called Paulistan Macrometropolis or *Macrometrópole Paulista*. It is not, as I thought, *São Paulo Metropolitan Area*, or *Região Metropolitana de São Paulo (RMSP)*, also called Large São Paulo *Grande São Paulo*. The difference in area and concepts can be grasped by checking out the Wikipedia pages [RMSP](https://pt.wikipedia.org/wiki/Regi%C3%A3o_Metropolitana_de_S%C3%A3o_Paulo#Munic.C3.ADpios) and [CME](https://pt.wikipedia.org/wiki/Complexo_Metropolitano_Expandido) for those who are interested. The important point to make is that this dataset is referring to the Expanded Metropolitan Complex which indeed includes parts of the Atlantic coast. This explains the occurrence of municiples and postcodes I did not expect, as well as the \"seamark\" tags, which I now understand are from the [Santos Seaboard](https://pt.wikipedia.org/wiki/Regi%C3%A3o_Metropolitana_da_Baixada_Santista).\n",
    "\n",
    "It also explains the occurrence of certain unexpected postcodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find({'address.postcode' : {'$regex' : '^1[2-9]'}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('5734c0c19a07ba122626c129'),\n",
       " u'address': {u'city': u'Jundia\\xed',\n",
       "  u'housenumber': u'865',\n",
       "  u'postcode': u'13201-905',\n",
       "  u'street': u'Rua XV de Novembro',\n",
       "  u'suburb': u'Centro'},\n",
       " u'amenity': u'hospital',\n",
       " u'created': {u'changeset': u'28654626',\n",
       "  u'timestamp': u'2015-02-06T15:45:48Z',\n",
       "  u'uid': u'1799626',\n",
       "  u'user': u'AjBelnuovo',\n",
       "  u'version': u'2'},\n",
       " u'data_prim': u'node',\n",
       " u'emergency': u'yes',\n",
       " u'id': u'677073968',\n",
       " u'name': u'Hospital Paulo Sacramento',\n",
       " u'pos': [-23.1890489, -46.8788723]}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find_one({'address.postcode' : {'$regex' : '^1[2-9]'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Postcode Format Inconsistencies\n",
    "\n",
    "Using the '$regex' operator, I was able to audit how postcodes format. By Brazilian convention, the format we should see is 'ddddd-ddd', or the regex ```'^([0-9]){5}([-])([0-9]){3}$'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9187"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find({'address.postcode' : {'$exists' : 1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8860"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find({'address.postcode' : {'$regex' : '^([0-9]){5}([-])([0-9]){3}$'}}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query shows there are some inconsistencies. I want to peek at 10 examples and see some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': ObjectId('573655fd9a07ba122647b066'),\n",
       "  u'address': {u'city': u'S\\xe3o Paulo',\n",
       "   u'postcode': u'010196-200',\n",
       "   u'street': u'Pra\\xe7a do Carmo',\n",
       "   u'suburb': u'S\\xe9'}},\n",
       " {u'_id': ObjectId('573655fe9a07ba122647ea6f'),\n",
       "  u'address': {u'housenumber': u'2022',\n",
       "   u'postcode': u'04345000',\n",
       "   u'street': u'Avenida Engenheiro Armando de Arruda Pereira'}},\n",
       " {u'_id': ObjectId('5736560d9a07ba12264ca6bd'),\n",
       "  u'address': {u'housenumber': u'5445',\n",
       "   u'postcode': u'12315280',\n",
       "   u'street': u'Avenida Juscelino Kubitschek'}},\n",
       " {u'_id': ObjectId('5736560f9a07ba12264d402b'),\n",
       "  u'address': {u'housenumber': u'137',\n",
       "   u'postcode': u'0454-000',\n",
       "   u'street': u'Rua Gra\\xfana'}},\n",
       " {u'_id': ObjectId('5736560f9a07ba12264d4e89'),\n",
       "  u'address': {u'housename': u'BurtiHD',\n",
       "   u'housenumber': u'974',\n",
       "   u'postcode': u'05311000',\n",
       "   u'street': u'Avenida Mofarrej'}},\n",
       " {u'_id': ObjectId('573656169a07ba12264f95f8'),\n",
       "  u'address': {u'housenumber': u'162',\n",
       "   u'postcode': u'01309000',\n",
       "   u'street': u'Rua Lu\\xeds Coelho'}},\n",
       " {u'_id': ObjectId('573656169a07ba12264f95f9'),\n",
       "  u'address': {u'housenumber': u'806',\n",
       "   u'postcode': u'05006000',\n",
       "   u'street': u'Rua Turiass\\xfa'}},\n",
       " {u'_id': ObjectId('573656169a07ba12264f95fa'),\n",
       "  u'address': {u'housenumber': u'404',\n",
       "   u'postcode': u'01309010',\n",
       "   u'street': u'Rua Ant\\xf4nio Carlos'}},\n",
       " {u'_id': ObjectId('573656179a07ba12264fb2fc'),\n",
       "  u'address': {u'housenumber': u'744',\n",
       "   u'postcode': u'01304001',\n",
       "   u'street': u'Rua Augusta'}},\n",
       " {u'_id': ObjectId('573656199a07ba12265072b4'),\n",
       "  u'address': {u'housename': u\"Condom\\xednio Edif\\xedcio OPUS D'ART\",\n",
       "   u'housenumber': u'905',\n",
       "   u'postcode': u'05025010',\n",
       "   u'street': u'Rua Raul Pomp\\xe9ia, 905'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [{'$match' : {'address.postcode' : { '$regex' : '^(?!^^([0-9]){5}([-])([0-9]){3}$).*$'}}},\n",
    "        { '$limit' : 10 }, \n",
    "        {'$project' : {'address' : 1 }}]\n",
    "list(sp.aggregate(pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there is a mix of incorrect format, such as '05025010' instead of '05025-010' and typos like an extra number or a missing one. My solution is in the first case, reformat, the second, discard. This was included in **shape_element**, and the data re-parsed and re-loaded into MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [{'$match' : {'address.postcode' : { '$regex' : '^(?!^^([0-9]){5}([-])([0-9]){3}$).*$'}}},\n",
    "        { '$limit' : 10 }, \n",
    "        {'$project' : {'address' : 1 }}]\n",
    "list(sp.aggregate(pipe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, a query for 10 postcodes that do not fit the format now returns an empty list, showing the problem was fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: A similar process *can* be done for phone numbers, however it is an intricate process for brazilian phone numbers are there are many different formats. (At this point numbers can have 8 to 10 digits excluding area code, there is an optional designation of operator, and other complicating issues.) This will require a little more time consuming research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Incorrect Postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': ObjectId('5734c1239a07ba122644a8aa'),\n",
      "  u'address': {u'city': u'S\\xe3o Paulo',\n",
      "               u'housenumber': u'456',\n",
      "               u'postcode': u'25450-000',\n",
      "               u'street': u'Rua Zilda',\n",
      "               u'suburb': u'Casa Verde'},\n",
      "  u'amenity': u'pharmacy',\n",
      "  u'building': u'yes',\n",
      "  u'created': {u'changeset': u'28762991',\n",
      "               u'timestamp': u'2015-02-10T23:31:28Z',\n",
      "               u'uid': u'1799626',\n",
      "               u'user': u'AjBelnuovo',\n",
      "               u'version': u'2'},\n",
      "  u'data_prim': u'way',\n",
      "  u'id': u'327507941',\n",
      "  u'name': u'BiFarma',\n",
      "  u'node_refs': [u'3342962713',\n",
      "                 u'3342962714',\n",
      "                 u'3342962715',\n",
      "                 u'3342962716',\n",
      "                 u'3342962713'],\n",
      "  u'phone': u'+55 11 3857 4511'}]\n"
     ]
    }
   ],
   "source": [
    "t = sp.find({'address.postcode' : {'$regex' : '^2'}})\n",
    "pprint.pprint(list(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above postcode is incorrect, it is supposed to be 02545-000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing Postcodes (CEPs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of addresses: 18224\n",
      "number of addresses with CEP: 9187\n",
      "number of addresses without CEP: 9037\n",
      "percentage of addresses missing CEP: 49 %\n"
     ]
    }
   ],
   "source": [
    "a = sp.find({'address' : {'$exists' : 1}}).count()\n",
    "b = sp.find({'address' : {'$exists' : 1}, 'address.postcode' : {'$exists' : 1}}).count()\n",
    "c = sp.find({'address' : {'$exists' : 1}, 'address.postcode' : {'$exists' : 0}}).count()\n",
    "print 'number of addresses:', a\n",
    "print 'number of addresses with CEP:', b \n",
    "print 'number of addresses without CEP:', c \n",
    "print 'percentage of addresses missing CEP:',int((float(c)/float(a))*100),'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Almost half of the addresses do not have postcodes (called \"CEP\" in Brazil). One follow-up project would be to scrape the CEPs from a reputable website (like Correios) and feed the CEPs back into the database. A good measure would be to obtain them by coordinates and by address both and analyse discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Overview of the Data\n",
    "\n",
    "This section contains statistical facts about the data, as well as the query used to determine it when applicable.\n",
    "\n",
    "** XML file size:** \n",
    "\n",
    "411,798 KB\n",
    "\n",
    "\n",
    "** JSON file:**\n",
    "\n",
    "is 472,242 KB\n",
    "\n",
    "\n",
    "** number of documents in the colection:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2168319"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** number of documents by data primitive:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261695 ways\n",
      "1900322 nodes\n",
      "6302 relations\n"
     ]
    }
   ],
   "source": [
    "cursor = sp.find({'data_prim' : 'way'})\n",
    "a = len(list(cursor))\n",
    "print a, 'ways'\n",
    "cursor = sp.find({'data_prim' : 'node'})\n",
    "b = len(list(cursor))\n",
    "print b, 'nodes'\n",
    "cursor = sp.find({'data_prim' : 'relation'})\n",
    "c =  len(list(cursor))\n",
    "print c, 'relations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** unique user ids:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp.distinct('created.uid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** the document that has the highest number of versions:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "[{u'ISO3166-1': {u'numeric': u'076'},\n",
      "  u'_id': ObjectId('573656729a07ba122666c255'),\n",
      "  u'admin_level': u'2',\n",
      "  u'alt_name': {u'vi': u'Brazil;Bra-xin'},\n",
      "  u'boundary': u'administrative',\n",
      "  u'created': {u'changeset': u'38321421',\n",
      "               u'timestamp': u'2016-04-05T15:33:47Z',\n",
      "               u'uid': u'1852029',\n",
      "               u'user': u'smaprs',\n",
      "               u'version': 445},\n",
      "  u'currency': u'BRL',\n",
      "  u'data_prim': u'relation',\n",
      "  u'driving_side': u'right',\n",
      "  u'flag': u'http://upload.wikimedia.org/wikipedia/commons/0/05/Flag_of_Brazil.svg',\n",
      "  u'id': u'59470',\n",
      "  u'name': {u'zh-classical': u'\\u5df4\\u897f'},\n",
      "  u'official_name': {u'zh': u'\\u5df4\\u897f\\u8054\\u90a6\\u5171\\u548c\\u56fd'},\n",
      "  u'old_name': {u'vi': u'Ba T\\xe2y'},\n",
      "  u'type': u'boundary',\n",
      "  u'wikidata': u'Q155',\n",
      "  u'wikipedia': u'pt:Brasil'}]\n"
     ]
    }
   ],
   "source": [
    "versions = sp.distinct('created.version')\n",
    "print max(versions)\n",
    "pprint.pprint(list(sp.find({'created.version' : max(versions)})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 'amenities' that occur the most, top 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'fuel', u'count': 1626},\n",
      " {u'_id': u'parking', u'count': 1231},\n",
      " {u'_id': u'restaurant', u'count': 997},\n",
      " {u'_id': u'school', u'count': 893},\n",
      " {u'_id': u'bank', u'count': 837},\n",
      " {u'_id': u'fast_food', u'count': 589},\n",
      " {u'_id': u'place_of_worship', u'count': 519},\n",
      " {u'_id': u'pharmacy', u'count': 384},\n",
      " {u'_id': u'pub', u'count': 310},\n",
      " {u'_id': u'bicycle_rental', u'count': 276}]\n"
     ]
    }
   ],
   "source": [
    "pipe = [{'$match' : {'amenity': {'$exists' : 1}}},\n",
    "        {'$group': {'_id': '$amenity', 'count': {'$sum': 1}}},\n",
    "        {'$sort' : {'count': -1}},\n",
    "        {'$limit' : 10},\n",
    "        {'$project' : {'amenity' : 1, 'count': 1}}\n",
    "        ]\n",
    "c = sp.aggregate(pipe)\n",
    "pprint.pprint(list(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**top 10 religions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'christian', u'count': 442},\n",
      " {u'_id': None, u'count': 56},\n",
      " {u'_id': u'buddhist', u'count': 6},\n",
      " {u'_id': u'muslim', u'count': 4},\n",
      " {u'_id': u'spiritualist', u'count': 3},\n",
      " {u'_id': u'jewish', u'count': 2},\n",
      " {u'_id': u'umbanda', u'count': 1},\n",
      " {u'_id': u'candombl\\xe9_na\\xe7\\xe3o_gege', u'count': 1},\n",
      " {u'_id': u'multifaith', u'count': 1},\n",
      " {u'_id': u'candombl\\xe9', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "pipe = [{'$match' : {'amenity': 'place_of_worship'}},\n",
    "        {'$group': {'_id': '$religion', 'count': {'$sum': 1}}},\n",
    "        {'$sort' : {'count': -1}},\n",
    "        {'$limit' : 10},\n",
    "        {'$project' : {'religion' : 1, 'count': 1}}\n",
    "        ]\n",
    "c = sp.aggregate(pipe)\n",
    "pprint.pprint(list(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows this data is severely incomplete and does not lend itself for statistics, not on religion, anyway. There are lenty more than 2 Jewish places of worship in Sao Paulo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** number of pizza places: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.find({'cuisine': 'pizza'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I am sure that it actually is more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Curiosity Queries\n",
    "\n",
    "This section contains queries that were performed motivated by personal curiosity. If the choices seem arbitrary, it is because they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many street names have a military rank in the name?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major : 75\n",
      "Capitão : 154\n",
      "Marechal : 90\n",
      "Tenente : 89\n",
      "General : 195\n",
      "Almirante : 49\n",
      "Brigadeiro : 34\n",
      "Contra Almirante : 1\n",
      "TOTAL: 687\n"
     ]
    }
   ],
   "source": [
    "def street_starts_with(letters):\n",
    "    \"\"\"takes a string and returns a regex string to be used with operator\n",
    "    $regex to query sp collections for streets starting with the string\"\"\"\n",
    "    a = ['Acostamento', \n",
    "         u'Pra\\xe7a', \n",
    "         'Alameda', \n",
    "         'Viela', \n",
    "         'Estrada', \n",
    "         'Rua', \n",
    "         'Acesso', \n",
    "         'Parque', \n",
    "         'Largo', \n",
    "         'Via', \n",
    "         'Marginal', \n",
    "         'Rodovia', \n",
    "         'Corredor', \n",
    "         'Viaduto', \n",
    "         'Travessa', \n",
    "         'Pateo', \n",
    "         'Avenida', \n",
    "         'Passagem',\n",
    "         u'Complexo Vi\\xe1rio']\n",
    "    expression = '|'.join(a)\n",
    "    return '^'+'(' + expression + ')' + ' ' + letters\n",
    "\n",
    "\"\"\" from wikipedia: Almirante\tMarechal\tMarechal do Ar\n",
    "Almirante de Esquadra\tGeneral de Exército\tTenente Brigadeiro do Ar\n",
    "Vice Almirante\tGeneral de Divisão\tMajor Brigadeiro\n",
    "Contra Almirante\tGeneral de Brigada\tBrigadeiro\n",
    "Capitão de Mar e Guerra\tCoronel\tCoronel\n",
    "Capitão de Fragata\tTenente Coronel\tTenente Coronel\n",
    "Capitão de Corveta\tMajor\tMajor\n",
    "Capitão Tenente\tCapitão\tCapitão \"\"\"\n",
    "\n",
    "military_ranks = ['Almirante',\n",
    "                  'Marechal',\n",
    "                  'Marechal',\n",
    "                  'General',\n",
    "                  'Tenente',\n",
    "                  'Brigadeiro',\n",
    "                  'Major',\n",
    "                  'Contra Almirante',\n",
    "                  u'Capitão']\n",
    "nomes = {}\n",
    "for rank in military_ranks:\n",
    "    nomes[rank] = set()\n",
    "    result =  sp.find({'data_prim' : 'way', 'name': {'$regex': street_starts_with(rank)}})\n",
    "    for r in result:\n",
    "        nomes[rank].add(r['name'])\n",
    "soma = 0\n",
    "for k in nomes:\n",
    "    soma += len(nomes[k])\n",
    "    print k, ':', len(nomes[k])\n",
    "print 'TOTAL:', soma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many street names start with X? And Z?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with X: 148\n",
      "Starting with Z: 195\n"
     ]
    }
   ],
   "source": [
    "x = sp.find({'data_prim':'way', 'name': {'$regex': street_starts_with('X')}}).count()\n",
    "print 'Starting with X:', x\n",
    "z = sp.find({'data_prim':'way', 'name': {'$regex': street_starts_with('Z')}}).count()\n",
    "print 'Starting with Z:', z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There are some unanswered questions that could lend themselves to further projects:\n",
    "\n",
    "* **What is the exact delimitations of the data and is it complying with official government designation for the Expanded Metropolitan Complex?**\n",
    "\n",
    "It proved harder than it seems to obtain such official designation. I fully intend to continue to pursue this question. Once the official information is at hand, it will not be hard to check against the data.\n",
    "\n",
    "* **How many street, schools or other public places have been named after officials of the military dictatorship under which Brazil was governed between 1964 and 1985?**\n",
    "\n",
    "This is a bigger project, requiring the scraping of quantities of information on the historical period. It is, however, feasible and an important question to be answered. Perhaps, initially it can be answered for the city only and then expand the question.\n",
    "\n",
    "Overall, the data lacks completeness and normalization. One important task is to add the postcodes to all the addresses, a plan for which is outlined above. Another idea for data validation is to check the data for consistency with information scraped from Correios website, which contains the official post office information in Brazil. Once the post office data is understood and one is able to perform efficient requests and scraping, the following validation tests can be performed:\n",
    " - postcodes from data vs POS \n",
    " - postcode from data vs street names\n",
    "\n",
    " \n",
    "This is powerful data and can be used in many different creative ways, such as using the POS information to make plots showing what is popular in each neighborhood, which ones have more parks, different types of churches. The possibilities are endless once the data is properly clean, normalized and validated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "* http://www.openstreetmap.org/relation/2661855#map=9/-23.6242/-46.4510\n",
    "* [several pages os OSM Wiki](https://wiki.openstreetmap.org/wiki/Main_Page)\n",
    "* https://pt.wikipedia.org/wiki/Regi%C3%A3o_Metropolitana_da_Baixada_Santista\n",
    "* https://pt.wikipedia.org/wiki/Complexo_Metropolitano_Expandido\n",
    "* https://pt.wikipedia.org/wiki/Regi%C3%A3o_Metropolitana_de_S%C3%A3o_Paulo#Munic.C3.ADpios\n",
    "* https://en.wikipedia.org/wiki/Expanded_Metropolitan_Complex_of_S%C3%A3o_Paulo\n",
    "* https://pt.wikipedia.org/wiki/Complexo_Metropolitano_Expandido\n",
    "* http://stackoverflow.com/questions/37014500/how-to-use-recursion-to-nest-dictionaries-while-integrating-with-existing-record\n",
    "* https://discussions.udacity.com/t/reducing-memory-footprint-when-processing-large-datasets-in-xml/37571/3\n",
    "* http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "* https://discussions.udacity.com/t/how-to-provide-sample-data-for-the-final-project/7118/13\n",
    "* https://discussions.udacity.com/t/valueerror-i-o-operation-on-closed-file/167469/6\n",
    "* https://discussions.udacity.com/t/keep-attr-atrr-atrr-formatted-data/166864/14\n",
    "* https://discussions.udacity.com/t/i-have-an-adequate-update-name-to-improve-street-names-not-sure-how-to-use-it/166569/5    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
